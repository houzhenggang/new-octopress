<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[分类: 云计算 | Hegel2011的博客]]></title>
  <link href="http://octopresszhangyu.herokuapp.com/blog/categories/云计算/atom.xml" rel="self"/>
  <link href="http://octopresszhangyu.herokuapp.com/"/>
  <updated>2012-08-16T10:48:31+08:00</updated>
  <id>http://octopresszhangyu.herokuapp.com/</id>
  <author>
    <name><![CDATA[Hegel 2011]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[hadoop集群安装手记]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2012/08/06/hadoopji-qun-an-zhuang-shou-ji/"/>
    <updated>2012-08-06T16:43:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2012/08/06/hadoopji-qun-an-zhuang-shou-ji</id>
    <content type="html"><![CDATA[<p>ssh的使用公钥登录配置</p>

<p><code>bash
ssh-keygen
ssh-copy-id -i ~/.ssh/id_rsa.pub 192.168.203.198
</code></p>

<p><strong>数据源的输入才是Hadoop方便地方</strong></p>

<p>定义一个数据源，可以是数据库也可以是文件夹，当然最好是hadoop的分布式文件夹<br/>
读取数据的划分是hadoop自动进行的，开发只需要定义要数据源就行<br/>
比如一个10000行的记录，如果有10个计算进程去做的话，hadoop会自动把数据切成1000行<br/>
然后你写的代码就是读入一行数据后要做些什么操作。比如说算总值或者平均值的话，就把数字全加起来，这个过程叫Map<br/>
然后10个计算进程都算出来了，需要汇总结果，汇总的进程就叫reduce<br/>
map/reduce之间传输的内容就是map的输出，一般也是一个key value，但是这里面具体赛什么是随意的</p>

<h1>文献</h1>

<p><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/">创建一个节点</a><br/>
<a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/">创建集群</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Replace log Files with Streams]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2012/06/05/replace-log-files-with-streams/"/>
    <updated>2012-06-05T12:16:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2012/06/05/replace-log-files-with-streams</id>
    <content type="html"><![CDATA[<p>早前读过adam的<a href="http://adam.heroku.com/past/2011/4/1/logs_are_streams_not_files/">Logs Are Stream, Not Files</a>, 但当时是阅读
整个博客，所以对细节不甚了了。今天为了写另一片关于计算模型的文章，顺便又看了一下此文，有新的收获。</p>

<p>日志是无穷无尽的流，而不是文件。文件只是流的某种最终形式。因为流是可以很容易的被其他程序继续利用（作为输入），而文件相对就困难一些。
总之，流更适合对分布式系统的集中处理。</p>

<h3>站在开发的角度</h3>

<p>以开发的角度来看，输出的内容无非就是stdout stderr，以及日志文件。但实际上文件和std很容易引起混淆。个人一直认为最直接最有用的
莫过于<code>puts</code> <code>cout</code> <code>System.out</code> <code>printf</code> ，logger.info logger.debug我并没有发现到底有多少作用。对于运行在服务端的程序，这
实在显得不是很有必要。stderr和stdout就足够用来区分了。</p>

<h3>站在部署的角度</h3>

<p>以部署的角度来看，全部当做stream自然好处很多。可以统一的重定向到文件或者syslog，或者其他更现代化的日志系统。只要这个系统能接受
一个input stream。</p>

<p>结论就是，能用流就用流吧，日志文件实在是很靠后的选择。</p>

<h3>几种技巧和工具</h3>

<ol>
<li><p><a href="http://www.cyberciti.biz/tips/howto-linux-unix-write-to-syslog.html">syslog</a> <br/>
syslog是要搭配bash的命令<code>logger</code>使用的。
<code>sh
 mydaemon | tee /var/log/mydaemon.log | logger
</code>
tee是让流再复制一份，logger则是linux自带的网syslog发消息的程序。发的目的地可以是远端的也可以是本地的，基于UDP协议。</p></li>
<li><p><a href="https://github.com/facebook/scribe/wiki">Scribe</a><br/>
则是facebook开源的日志工具，用法类似syslog，只是可以更多的组装，而且也提供了可以写日志的HTTP接口。</p></li>
<li><p><a href="https://github.com/heroku/logplex">Logplex</a><br/>
这是heroku的日志系统。基于erlang编写。顺便说一句，erlang和js都是很好的在语言级别实现了对Event-Driven I/O的深度整合。
存放库是Redis这个NoSQL数据库。根据heroku的架构，个人猜测是在启动dyno时，将日志信息重定向了给Logplex的客户端，客户端
再将内容加上app_id等信息后发给Logplex的服务。这是使用这项服务一个<a href="https://devcenter.heroku.com/articles/scaling#process_formation">帮助</a>.</p></li>
<li><p>Upstart launchd  Systemd
这是Ubuntu OSX 下替换传统的linux的init.d，负责启动服务的后台库。</p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The difference between MapReduce and Hadoop]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2012/03/27/the-difference-between-mapreduce-and-hadoop/"/>
    <updated>2012-03-27T12:42:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2012/03/27/the-difference-between-mapreduce-and-hadoop</id>
    <content type="html"><![CDATA[<p>简单地说，MapReduce是一种计算模型，也就是一种设计；而Hadoop则是这种模式的Java实现，由Apache发行。
2004年的时候，google发布了MapReduce的论文，并给出了一个C++的实现。此后，很多种语言都实现了MapReduce模式，比较有名的有，
Hadoop(java), Skynet(ruby)等。<br/>
而Hadoop除了这个计算模型外，还包括分布式数据库和分布式内存存储两大部分内容。</p>

<p>MapReduce实际上是切分+管道技术。可以表述称 Map | Reduce。
通过Map，将要处理的数据转换成[key, value], 再管道给Reduce继续进行处理。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Heroku的新服务]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2011/07/07/new-service-offered-by-heroku-nodejs/"/>
    <updated>2011-07-07T18:48:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2011/07/07/new-service-offered-by-heroku-nodejs</id>
    <content type="html"><![CDATA[<p>Heroku发表了一系列介绍自己内部升级的文章。最重要的变化是开始支持除ruby/rack以外的语言和框架。起先加入了node.js，然后又加入了Clojure. Clojure是跑在jvm上面的一种语言，对这块没研究。</p>

<p>在heroku看来，ruby的高动态性和强调美感使她天生适合面向用户的web应用。Node.js的基于事件的搞并发使得它适合实时web。 Clojure使得需要correctness,performance,composability, optionally的组件成为可能，并且触及java的生态圈。</p>

<p>heroku开始支持Java. <a href="http://blog.heroku.com/archives/2011/8/25/java/?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%3A+heroku+%28Heroku+News%29">heroku for java</a>
不支持war包比较让人痛苦。试着发布一个应用玩玩吧。</p>

<p>Real apps come from real developers Real developers wanted to use their own tools</p>
]]></content>
  </entry>
  
</feed>
