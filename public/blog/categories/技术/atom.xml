<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[分类: 技术 | Hegel2011的博客]]></title>
  <link href="http://octopresszhangyu.herokuapp.com/blog/categories/技术/atom.xml" rel="self"/>
  <link href="http://octopresszhangyu.herokuapp.com/"/>
  <updated>2012-12-09T21:16:19+08:00</updated>
  <id>http://octopresszhangyu.herokuapp.com/</id>
  <author>
    <name><![CDATA[Hegel 2011]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[What's new in Rails 4]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2012/11/23/whats-new-in-rails-4/"/>
    <updated>2012-11-23T22:00:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2012/11/23/whats-new-in-rails-4</id>
    <content type="html"><![CDATA[<p>来自<a href="http://vimeo.com/51181496">视频</a>的笔记</p>

<h3>Rails.queue</h3>

<p>```
class ExpensiveOperation
  def run</p>

<p>  end
end</p>

<p>Rails.queue.push(ExpensiveOperation.new)</p>

<p>```
config.queue = Sidekiq::Client::Queue #Resque, delayed_job</p>

<h3>strong_parameters</h3>

<p>Alternative to MassAssignmentSecurity</p>

<p>```
class PostsController &lt; ApplicationController
  def create</p>

<pre><code> @post = Post.create(post_params)
</code></pre>

<p>  end</p>

<p>  private
  def post_params</p>

<pre><code>params.require(:post).permit(:title, :body)
</code></pre>

<p>  end
end</p>

<p>```</p>

<h3>Turbolinks</h3>

<h3>Cache Digests(Russian Doll Caching)</h3>

<p>```</p>

<h1>app/views/projects/show.html.erb</h1>

<p>&lt;% cache project do %>
  <h1> </h1>
  &lt;%= render project.documents %>
&lt;% end %>
```</p>

<h3>ActionController::Live</h3>

<p>```
include ActionController::Live</p>

<p>response.stream.write "hello\n"</p>

<p>```</p>

<h3>PATCH Verb</h3>

<p>just as PUT</p>

<h3>Plugins are dead; long live plugins</h3>

<p>use gems with bundle</p>

<h3>lambda scope</h3>

<p>```
scope :published, -> { where(:published => true) }</p>

<h1>Pitfall</h1>

<p>scoped :recent, where("created_at > ?", 1.week.ago)
```</p>

<p>@sikachu</p>

<p>Routing Concerns</p>

<p>Long live activeresource</p>

<p>The changes seem not to be surprising compared with those from Rails 2.3 to Rails 3.0. And perhaps less than from 3.0 to 3.1.
They are real slight for Rails. I guess this is due to DHH have just had a Colt HH. The same for me!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[hadoop集群安装手记]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2012/08/06/hadoopji-qun-an-zhuang-shou-ji/"/>
    <updated>2012-08-06T16:43:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2012/08/06/hadoopji-qun-an-zhuang-shou-ji</id>
    <content type="html"><![CDATA[<p>ssh的使用公钥登录配置</p>

<p><code>bash
ssh-keygen
ssh-copy-id -i ~/.ssh/id_rsa.pub 192.168.203.198
</code></p>

<p>=======
<strong>数据源的输入才是Hadoop方便地方</strong></p>

<p>定义一个数据源，可以是数据库也可以是文件夹，当然最好是hadoop的分布式文件夹<br/>
读取数据的划分是hadoop自动进行的，开发只需要定义要数据源就行<br/>
比如一个10000行的记录，如果有10个计算进程去做的话，hadoop会自动把数据切成1000行<br/>
然后你写的代码就是读入一行数据后要做些什么操作。比如说算总值或者平均值的话，就把数字全加起来，这个过程叫Map<br/>
然后10个计算进程都算出来了，需要汇总结果，汇总的进程就叫reduce<br/>
map/reduce之间传输的内容就是map的输出，一般也是一个key value，但是这里面具体赛什么是随意的</p>

<h1>文献</h1>

<p><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/">创建一个节点</a><br/>
<a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/">创建集群</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数学之美]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2012/07/02/shu-xue-zhi-mei/"/>
    <updated>2012-07-02T23:47:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2012/07/02/shu-xue-zhi-mei</id>
    <content type="html"><![CDATA[<p>在京东买的六本书都到了.这次的书普遍不是很厚.当然,3本项目管理的书加起来还是很厚的.也要有600多页.</p>

<p>分了2本给同事,自己抱起&lt;数学之美>先看了起来. 花了一周的时间, 这本书基本看完了. 先说说篇幅.</p>

<p>正文是256页,但是排版是很松的, 有点像经管类书籍的排版, 所以这本书的篇幅并不大.主要是27个章节, 实际就是27片文章, 大部分的来源是google黑板报.
吴军博士深入浅出的能力很强,所以大部分时候不需要大家去推算数学公式, 他会把公式的含义和影响解释的很清楚. 这是他的本事. 通篇来看, 文章大致可以
分成下面几类:</p>

<ul>
<li>信息论的基础知识<br/>
这部分主要包括第一章的信息, 文字和数字,  有些细节很有趣, 比如罗马数字原来里面含有加减法的含义</li>
<li>数学原理的应用<br/>
这个主要是以google在语言分析 语音分析 拼音输入法 是集中体现了马尔科夫链和统计学; 余弦定理与新闻分类/文章分类/自动分类</li>
<li>google的技术<br/>
数学知识只是用于帮助解决计算复杂度, 而不是作为解题的思路和理论基础.集中体现在PageRank, TF-IDF中</li>
<li>密码学的知识<br/>
当然,密码学本身也用到了数学尤其是数论的内容.包括用指纹减少信息量,提高存储和比对的效率,</li>
<li>一些知名的算法<br/>
主要有维特比</li>
<li>名人传记<br/>
这部分总有自抬身价的意思.特别是开复开复的出现很多, 也算是sns互吹了.有特色的就是这些专家大都还处于这个时代,相当有新鲜感,
当然,维特比这个算法也被当做经典算法我只能说很多东西其实运气更加重要.第一个发现的人得到特别的奖赏也是应该的.</li>
</ul>


<p>整本书有些部分是让我第一次了解了语音/文章识别用到的其实都概率推测的东西, 以及google的页面排序和TF-IDF这些指标产生. 而有些内容则触发为进一步
了解其他知识.比如昨天刚刚搞懂的频分/时分/码分复用, 这东西第一次在网络课上听到已经是十二三年前了, 即便考研的时候也未弄懂, 而这次在介绍维特比
和cdma的时候有看见了,然后再看看谢希仁的教材,竟然就懂了. 那个向量正交真的是很精巧的设计, 这样才使得接收端收到叠加信息后还能还原出来.</p>

<p>除了介绍一些新颖的做法和能引发看其他内容的兴趣, 文章也提到了一些google的文化甚至美国工程师的文化.实际上无论美国还是中国,第一流的工程师还是
向往在大公司工作, 只是美国有那么一部分喜欢创业的工程师, 中国这个比例少一些而已.否则google也不会起先山寨遍地后来又系统化地解决问题.其实两边
一流的工程师或者某个领域的权威专家都是愿意给大公司打工,也只有大公司才养得起这些高手吧.</p>

<p>整本书让人<strong>眼界打开了不少</strong>, 也能<strong>引起自己学习其他内容的兴趣</strong>, 这当然就是一本<strong>好书</strong>了. 不过,这本书的内容毕竟讲的都比较浅显, 并不是一本很耐看的书.
属于速食图书里面的营养品.提取一下主题,感兴趣的还是要自己继续找材料摸索练习才行.但是无论如何,这都是一本好书,而且很值得一看,投入产出比相当高
的一本书.否则也不值得为了它写一篇书评.</p>

<p>此外, 就是第一次了解了吴军博士. 他的履历确实相当漂亮, 文字写的也很棒, 这样的人才应该是<strong>中国IT业界的领军</strong>
人物.才是中国一流大学工程师的代表.</p>

<p>最后是分类做的一些<strong>笔记</strong>.</p>

<ul>
<li>科普</li>
</ul>


<p>文字按照意思有<strong>聚类</strong>, 类似机器学习的聚类, 副作用是有歧义性.</p>

<p>罗塞塔石碑上3种文字提供了冗余和翻译的基础.</p>

<p>数字从文字中被单独抽象出来表达.大部分是十进制的, 不过 也有玛雅人用的20进制,所以他们的一个世纪是400年.<br/>
中国数字的编解码规则是乘法.</p>

<p>托勒密的60个圆的点球模型,实现了365天+4年一润的模型.格里格拉教皇对日历闰年的调整,世纪末最后一年的闰年取消,每400年加一个闰年.
哥白尼的日心说用来更少的圆8-10个,但误差极大,直到开普勒改成这几个椭圆才最终令人信服.牛顿最后又加上了万有引力.</p>

<ul>
<li>马尔科夫与动态路径</li>
</ul>


<p>自然语言的处理从<strong>设立规则</strong>发展到了<strong>统计</strong>算概率, 就是对最终结果进行对比, 这个过程走的很漫长.<br/>
语音搜索和自动翻译用的技术是一样的,全部用的统计语言模型.最后落实到出现的次数比 P(wi|wi-1) = #(wi-1, wi)/#(wi-1)
分词也是一样的,目的就是计算出每种分词后概率最大的那个.</p>

<p>马尔科夫是用于概率简化,把前面n个条件简化成一个或者稍多一点的.</p>

<p>动态规划其实就是一种维特比算法, 然后再加入按group分成队列的话, 就能应付大部分请求了.<br/>
拼音输入法的原理也是概率统计推断, 从起点到终点找出最短路径.<br/>
条件随机场与句法分析,让被分析的内容成为句子的概率最大.<br/>
维特比和维特比优化路径微观算法.</p>

<ul>
<li>PageRank<br/>
信息熵: log32=5, 其实就是需要几位bit才能表示状态, 这也决定了搜索所需要的次数.
<code>H=-(p1.logp1 + p2.logp2 + ... + p32.logp32)</code></li>
</ul>


<p>信息的作用是<strong>消除一部分不确定性</strong>, 通过寻找<strong>相关的</strong>信息来消除.自然语言的处理就是寻找相关信息的过程.</p>

<p>搜索引擎的基本原理是对关键字做索引,然后用布尔代数计算结果.</p>

<p><strong>网页质量</strong>信息 &amp; <strong>网络相关性</strong>信息<br/>
质量是根据被引用的链接数量来做排序因子的, 可以看出他们是有学术背景的. Page眼中,页面就是一个节点, 链接就像一个弧, 把互联网用矩阵来进行描述.
排名公式:
<code>Bi = AxBi-1</code><br/>
其中Bi是N维列向量, A是一个N维方矩阵, 将上公式反复迭代10次,可得到收敛的结果,起初和假定B的每个数字都是1/N. A的值则是各元素(各网页)之间的链接
数字列表.<br/>
单文本词频(Term Frequency), 是在文章中出现的次数/总词汇数. 简称<strong>TF</strong><br/>
实际应用时又有加权处理.权重大小根据该词在所有页面中出现的次数来确定,出现多的权重就小,出现少的权重就大. 简称<strong>IDF</strong>, 公式为log(D/Dw).<br/>
<code>Tf-IDF = TF1xIDF1 + ... + TFNxIDFN</code><br/>
IDF之所以使用Log,其实就是采用每个词的信息量作为权重,而根据香农的熵公式,I(w) = -P(w)logP(w)</p>

<ul>
<li>向量相关</li>
</ul>


<p><strong>余弦定理与新闻分类</strong>,实质就是计算两个向量的夹角, 以明确相关度.计算公式就是两个向量内积然后除以向量模的乘积.</p>

<p>反SEO的一些内容,本质就是要把噪音去除, 比如把出站链接相似的网站屏蔽</p>

<p>CDMA技术. 海蒂拉马尔发明的实际上是频率跳变的码流, 然后才被用来当做复用. 这里面正交向量当做code的设计思路使得多种信号在叠加后也能按各个code分别进行还原, 这个整个设计最
重要的地方.也在此,进一步明白了频分/时分/码分的区别和特点.吴军至少写书的时候还不懂cdma背后的数学原理,否则他肯定会谈的.</p>

<p>文本自收敛分类.其实就是不停迭代计算向量.使得各类内部的距离d最短,而类间距离D最长.</p>

<ul>
<li>随机数的映射</li>
</ul>


<p>信息指纹的核心思想是通过随机数函数把内容转换为较短的随机数,这样可以用来做判定集合相同/反盗版/判定集合基本相同.即提取对应的信息保持足够简短,
这样的好处是存放和检索的成本大大下降.MD5的重复概率只有一千八百亿亿次,2的六十四次方.</p>

<p>布隆过滤器.其实就是把原始信息投射到一个几亿字节的几位中去.然后通过判断这几位是否为1,来明确后来的信息是否在要过滤的集合中.本质上就是一种随机数的算法.弱点是有一定的假阳性.</p>

<ul>
<li>其他</li>
</ul>


<p>爬虫其实就是按图论进行遍历.欧拉七桥的图论定理.每一个顶点的度必须是偶数.具体有广度优先还是深度有限?页面分析和URL提取?URL如何存放的瓶颈?</p>

<p>密码学基础,公钥私钥</p>

<h4>人物</h4>

<p>贾里尼克,捷克犹太人,吴军美国大学语言语音处理中心的创始人<br/>
马库斯,自然语言转向统计方式处理的另一个教父级别的人物, 还建立了语料库,让大家有一个平等的环境进行算法比较测试<br/>
辛格博士, AK47, 简单有效</p>

<h3>推荐书单</h3>

<p><a href="http://202.120.227.11/F/DN9IEV1ANU9DYR3D8YYF2FX84MGGMV6U9B37KVBLASBNGDQPNB-55367?func=item-global-exp&amp;doc_number=000573910&amp;item_sequence=000010&amp;sub_library=LJCXK">信息论基础</a> Cover<br/>
<a href="http://infolab.stanford.edu/~backrub/google.html">google.html</a><br/>
地址分析的有限状态机编写,还有基于概率的有限状态机<br/>
相似hash的算法(Simhash)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Programming Language Performance]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2012/06/26/programming-language-performance/"/>
    <updated>2012-06-26T10:06:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2012/06/26/programming-language-performance</id>
    <content type="html"><![CDATA[<p><img src="/images/screen_print/programming_performance.jpg" alt="语言性能排名" /></p>

<p>JavaScript是一个很大的亮点。V8的性能确实快。
相对于Ruby Python的eveneted-driven编程库，JS的优势在于：<br/>
1. 不单网络模型是事件型的，基本上所有的库都是事件型的API；<br/>
2. 它的执行速度要比其他脚本语言快的多的多，比ruby python的执行速度平均快15x，甚至比erlang还要快5x。
而且在可以预见的5-10年内，这个优势应该还会继续扩大。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Replace log Files with Streams]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2012/06/05/replace-log-files-with-streams/"/>
    <updated>2012-06-05T12:16:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2012/06/05/replace-log-files-with-streams</id>
    <content type="html"><![CDATA[<p>早前读过adam的<a href="http://adam.heroku.com/past/2011/4/1/logs_are_streams_not_files/">Logs Are Stream, Not Files</a>, 但当时是阅读
整个博客，所以对细节不甚了了。今天为了写另一片关于计算模型的文章，顺便又看了一下此文，有新的收获。</p>

<p>日志是无穷无尽的流，而不是文件。文件只是流的某种最终形式。因为流是可以很容易的被其他程序继续利用（作为输入），而文件相对就困难一些。
总之，流更适合对分布式系统的集中处理。</p>

<h3>站在开发的角度</h3>

<p>以开发的角度来看，输出的内容无非就是stdout stderr，以及日志文件。但实际上文件和std很容易引起混淆。个人一直认为最直接最有用的
莫过于<code>puts</code> <code>cout</code> <code>System.out</code> <code>printf</code> ，logger.info logger.debug我并没有发现到底有多少作用。对于运行在服务端的程序，这
实在显得不是很有必要。stderr和stdout就足够用来区分了。</p>

<h3>站在部署的角度</h3>

<p>以部署的角度来看，全部当做stream自然好处很多。可以统一的重定向到文件或者syslog，或者其他更现代化的日志系统。只要这个系统能接受
一个input stream。</p>

<p>结论就是，能用流就用流吧，日志文件实在是很靠后的选择。</p>

<h3>几种技巧和工具</h3>

<ol>
<li><p><a href="http://www.cyberciti.biz/tips/howto-linux-unix-write-to-syslog.html">syslog</a> <br/>
syslog是要搭配bash的命令<code>logger</code>使用的。
<code>sh
 mydaemon | tee /var/log/mydaemon.log | logger
</code>
tee是让流再复制一份，logger则是linux自带的网syslog发消息的程序。发的目的地可以是远端的也可以是本地的，基于UDP协议。</p></li>
<li><p><a href="https://github.com/facebook/scribe/wiki">Scribe</a><br/>
则是facebook开源的日志工具，用法类似syslog，只是可以更多的组装，而且也提供了可以写日志的HTTP接口。</p></li>
<li><p><a href="https://github.com/heroku/logplex">Logplex</a><br/>
这是heroku的日志系统。基于erlang编写。顺便说一句，erlang和js都是很好的在语言级别实现了对Event-Driven I/O的深度整合。
存放库是Redis这个NoSQL数据库。根据heroku的架构，个人猜测是在启动dyno时，将日志信息重定向了给Logplex的客户端，客户端
再将内容加上app_id等信息后发给Logplex的服务。这是使用这项服务一个<a href="https://devcenter.heroku.com/articles/scaling#process_formation">帮助</a>.</p></li>
<li><p>Upstart launchd  Systemd
这是Ubuntu OSX 下替换传统的linux的init.d，负责启动服务的后台库。</p></li>
</ol>

]]></content>
  </entry>
  
</feed>
