<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[分类: 读书笔记 | Hegel2011的博客]]></title>
  <link href="http://octopresszhangyu.herokuapp.com/blog/categories/读书笔记/atom.xml" rel="self"/>
  <link href="http://octopresszhangyu.herokuapp.com/"/>
  <updated>2012-11-24T00:07:25+08:00</updated>
  <id>http://octopresszhangyu.herokuapp.com/</id>
  <author>
    <name><![CDATA[Hegel 2011]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Learning Mongodb (3) 部署和管理]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2012/04/17/deployment-and-administration-for-mongod/"/>
    <updated>2012-04-17T11:07:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2012/04/17/deployment-and-administration-for-mongod</id>
    <content type="html"><![CDATA[<p>系列链接：</p>

<ol>
<li><a href="/blog/2012/03/29/learning-mongodb-1/">mongodb基础</a></li>
<li><a href="/blog/2012/03/30/learning-mongodb-2/">尝试分布式及复制  （sharding and replication)</a></li>
<li><a href="/blog/2012/04/17/deployment-and-administration-for-mongod/">部署和管理</a></li>
</ol>


<h3>部署环境要求</h3>

<ul>
<li><p>架构</p>

<ul>
<li><strong>64-bit</strong> , mongo会把所有的文件都map成虚拟地址空间，32位至多只能利用2GB内存空间</li>
<li>little-endian , 不适合SPARC, PowerPC, PA-RISC，客户端（驱动）无所谓运行环境</li>
</ul>
</li>
<li><p>CPU</p>

<ul>
<li>非cpu密集型应用</li>
<li>I/O瓶颈远比CPU瓶颈出现的可能性多</li>
<li>多客户端同时读时，多核会被利用</li>
<li>写操作还是单核</li>
</ul>
</li>
<li><p>RAM</p>

<ul>
<li>越多越好</li>
<li>working set， index size， ram总数</li>
</ul>
</li>
<li><p>Disk</p>

<ul>
<li>IOPS 优先</li>
<li><em>background flush</em> 60s sync到磁盘一次</li>
<li>ssd可以优化速度（solid state drive）</li>
<li>RAID 10 组成 LVM</li>
</ul>
</li>
<li><p>文件系统</p>

<ul>
<li>ext4 或者 xfs，快速、连续的磁盘分配</li>
<li>禁用access time(atime)更新</li>
</ul>
</li>
</ul>


<p>```sh</p>

<h1>禁用 atime更新</h1>

<p>sudo mv /etc/fstab /etc/fstab.bak
sudo vim /etc/fstab</p>

<h1>file-system mount type options dump pass</h1>

<p>UUID=8309beda-bf62-43 /ssd ext4 noatime 0 2</p>

<p>改完文件后，可以不用重启
```</p>

<ul>
<li>文件描述符(FD)

<ul>
<li>默认的1024不够用  <code>lsof | grep mongo | wc -l</code> 可以查看当前mongod打开了多少个文件和连接</li>
<li>调大 <code>ulimit -Hn</code> 可以查看</li>
</ul>
</li>
</ul>


<p><code>sh
vi /etc/security/limits.conf
mongodb hard nofile 10240
下次登录后生效
</code></p>

<ul>
<li><p>时钟</p>

<ul>
<li>ntp协议，在跑shard和replication时候必须启用的东西</li>
</ul>
</li>
<li><p>ec2上使用mongodb</p>

<ul>
<li>ec2易用、地理范围广、价格有竞争力</li>
<li>68GB RAM的限制</li>
<li>使用  EBS 存储， 吞吐性能一般化</li>
</ul>
</li>
</ul>


<h3>服务器配置</h3>

<ul>
<li><p>选择拓扑</p>

<ul>
<li>是否需要shard， 是否需要replica</li>
<li>建议多机</li>
</ul>
</li>
<li><p>打开journal标志</p></li>
</ul>


<h3>安全保障</h3>

<ul>
<li><p>环境安全</p>

<ul>
<li>防火墙</li>
<li>数据的传输是不加密的</li>
</ul>
</li>
<li><p>授权</p>

<ul>
<li>给admin区增加用户<code>use admin; db.addUser("boss", "supersecret")</code></li>
<li>启动时带上选项 <code>mongod --auth</code></li>
<li>使用admin然后再授权 <code>use admin; db.auth("boss", "supersecret")</code></li>
<li>给其他库，如stock，创建用户 <code>use stocks; db.addUser("trader", "moneyfornuthin")</code></li>
<li>查找用户 <code>db.system.users.find()</code></li>
</ul>
</li>
<li><p>keyFile</p>

<ul>
<li>cluster 和 sharding 的时候需要制定keyFile，mongos实例也要具备这个密钥文件</li>
</ul>
</li>
<li><p>日志记录</p>

<ul>
<li>运行时--logpath指定</li>
</ul>
</li>
</ul>


<h3>监控工具</h3>

<ul>
<li><p>serverStatus<br/>
<code>use admin; db.runCommand({serverStatus: 1})</code><br/>
<strong>globalLock</strong> 表示等待写锁的时间。ratio过高意味着需要优化调整<br/>
<strong>mem</strong> 段的单位是MB</p></li>
<li><p>mongostat</p>

<ul>
<li>类似 iostat 运行的效果</li>
</ul>
</li>
<li><p>web console</p>

<ul>
<li>--rest 标志打开的话，可以从web处得到更多的运行信息</li>
</ul>
</li>
</ul>


<h3>备份与压缩</h3>

<p>mongodump 和 mongorestroe两个组合，也可以直接拷贝文件。不过后者要求lock数据库，即要求mongod停止往磁盘同步。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PM培训课程小结]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2012/04/15/lessons-from-a-training-on-pm/"/>
    <updated>2012-04-15T13:42:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2012/04/15/lessons-from-a-training-on-pm</id>
    <content type="html"><![CDATA[<p>趁着这个集成项目经理培训的五天培训，把辅导材料基本过了一遍。结合一些自己的经验，以及看<a href="http://xdite-smalltalk.tumblr.com/post/18034694291">别人</a>的一些文章，谈谈
自己的想法。</p>

<p>整本培训洋洋洒洒地讲了很多东西，客观的说能全面解析出这样一套体系的人相当的不简单。但是，这套体系应该说还是基于瀑布模型的，只不过
把大瀑布分拆成了n个小瀑布。实际上对软件开发并不太适用。不过因为整套体系认为<strong>光有集成</strong>是没有自己的竞争力的，要求有<strong>30%</strong>的软件开发内容，
于是又把软件开发放在里面。这也让我终于明白了工作的第一个部门成立的<strong>指导思想来源</strong>。因为我曾经相当一段时间纳闷为什么非要把系统集成
和研发凑合成一个部门。而且整个软件组还受集成一侧领导。这样能做出什么好东西来。但现在终于了解了这样组织的来源，而且以集成为主导也是因为这里面
要求软件比例不低于30%。</p>

<p>读了这本材料，终于明白公司是抄自国家认证考试的体系。也可能是其他人这么建议的，而也受了这个认证体系理论的影响。</p>

<p>软件开发有其自身的特点和重点。集成体系用其无所不包、范围够广的方式（9大范围）强行把软件包裹了进去。尽管看上去纳入了，但重点很不突出。
毕竟即使有软件工程也和系统工程是两码事情。所以，这次培训最大的结果是了解集成到底是什么思路，然后就通过这个考试，剩下的也只有自己继续摸索。</p>

<p>先说说集成这套体系。</p>

<p>首先，体系强调章程和计划。章程只有一个，而计划各个领域可以有很多。实际上，<strong>计划也是章程</strong>。只不过灵活度比章程强，可以按照需要进行
变更、更新，而章程定下了就不怎么更新了。计划或者章程规定了要做哪些事情，什么时候做，相关人员是什么，什么情况下需要变更。
随后就是一堆输入输出的交付物。</p>

<p>接下去的东西就好理解了，反正整个东西追求的结果都是文档。当然，文档确实有很多种，有些方法对保证系统也确实有帮助。
比如需求管理中的use case，每个case其实还是要用文字说明的，case图可以让需求方和系统分析师把东西交流清楚。但本质上就是
一个list，只是说明参与对象是什么。WBS分解，可以用于进一步细化和确认需求，分写出要做的工作。这些都是和软件开发业密切相关的，
但是在此之外，其他的东西实质上和软件的关联就不怎么大了。</p>

<p>最后，就是强调一切东西都要有确认，变更等都要走流程。这些东西如果乙方有能力实施，当然是很好的。</p>

<p>此外，还有一些东西就是骗来骗去的了。比如进度估计、成本估计，实际上即使传统的工程，能准确估算出来的都是很少的。</p>

<p>经典的启动、计划、执行、监控和收尾，又叫做PDCA（Plan Doing Check Action), 典型的普遍模型，自上而下。只是又可以分解成n个小瀑布。
但是，这个真的是软件开发里面最重要的事情嘛？</p>

<p>恐怕不是。</p>

<p>就我看来，软件或者互联网应用更多的强调对自身功能和模块的拆分，启动时往往很难明确细化东西。即使建筑工程本身，也就是这套体系产生的原型，
也有众多管理失败的案例，巴拿马运河等，那么软件对这套系统的适应性恐怕更难。而且这套体系显然没考虑很多项目管理工具的使用，如redmine、如版本控制系统，
而不同的工具或进步的工具是可以颠覆整个开发行为的。条条框框和优秀的软件结合起来会完全改变制造方式。与此相比，管理体系本身就显得僵化。当然，值得吸取的部分还是有的。
比如我们需要一个章程，需要需求的确认。</p>

<p>最最重要的，就是list要做的事情，给出最终时间。WBS后，确认细节，排好顺序，随后和用户确认。</p>

<p>而在这中间，系统的调整能力和团队和用户沟通的软实力才是起决定作用的东西。</p>

<p>Update(2012-05-15): 读了<a href="http://blog.xdite.net/posts/2012/05/13/the-startup-owners-manual-02/?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%3A+xxddite+%28Blog.XDite.net%29">New product Introduction Model的九宗罪</a>
进一步明确了PM这套东西不适合的原因。确实，这套东西适合的是“在有確定用戶，確定市場，確定 bussiness model 的情況下才能使用。也只有在這樣的情況下才有機會成功。”
“這很大程度了解釋了為什麼：個人、大公司要『新創』一個事業很容易失敗。而一些『大公司』要『山寨』一個服務也有機會取得成功。” <br/>
那么反过来，对于不够明确的、大家都还没谱的、都需要摸索的项目就不合适。<br/>
所谓<strong>执行</strong>是针对有成熟套路的东西，考察的是系统的熟练程度和配合程度。而创新、新业务是需要<strong>试错的</strong>，其中的核心能力是<strong>学习</strong>、<strong>摸索</strong>、<strong>观察</strong>和<strong>勇气</strong>。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Learning Mongodb (2)]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2012/03/30/learning-mongodb-2/"/>
    <updated>2012-03-30T17:03:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2012/03/30/learning-mongodb-2</id>
    <content type="html"><![CDATA[<p>系列链接：</p>

<ol>
<li><a href="/blog/2012/03/29/learning-mongodb-1/">mongodb基础</a></li>
<li><a href="/blog/2012/03/30/learning-mongodb-2/">尝试分布式及复制  （sharding and replication)</a></li>
<li><a href="/blog/2012/04/17/deployment-and-administration-for-mongod/">部署和管理</a></li>
</ol>


<p><strong>Replica sets</strong> vs <strong>Master-Slave</strong><br/>
目前已全面倒向支持前者。  前者是后者的加强，但限制是最大的1个set集群只有<strong>12</strong>个节点。</p>

<h3>Replica sets</h3>

<p>由3个nodes组成：</p>

<ol>
<li>mongod</li>
<li>mongod</li>
<li>arbiter（仲裁者）</li>
<li>其实未必需要arbiter，只是要求至少3个mongod的节点，arbiter只是不用写数据</li>
</ol>


<p>架设代码：<br/>
``` bash
mkdir /data/node1
mkdir /data/node2
mkdir /data/arbiter</p>

<p>numactl --interleave=all mongod --replSet pcdm --dbpath /data/node1 --port 40000 --oplogSize 5000
numactl --interleave=all mongod --replSet pcdm --dbpath /data/node2 --port 40001 --oplogSize 5000
numactl --interleave=all mongod --replSet pcdm --dbpath /data/arbiter --port 40002 --oplogSize 5000</p>

<p>./bin/mongo ngntest:40000</p>

<blockquote><p>rs.initiate()
config =  {_id: 'pcdm', members: [</p>

<pre><code>                      {_id: 0, host: 'localhost:40000'},
                      {_id: 1, host: 'localhost:40001'},
                      {_id: 2, host: 'localhost:40002'}]
       }
</code></pre>

<p>rs.initiate(config)
```</p></blockquote>

<ul>
<li>oplog and oplogSize</li>
</ul>


<p><code>oplogSize</code> 是replica里面同步日志的大小，默认是空闲磁盘的5%, 增加限制有利于磁盘空间的实际使用率，这其实是一个capped collection, 存放在叫 <strong>local</strong> 的空间中 ,会重复使用。<br/>
基本机制是这样的，主节点不停的往oplog里面写，从节点会去获取并写入本地的oplog，然后再‘应用'.</p>

<ul>
<li>heartbeat</li>
</ul>


<p>各节点之间会定期进行心跳交流， <em>如果primary发现四处都不通，则会自动降级为secondary</em> 。 也因此， 整个set需要3个来起步。另外一个其实起的就是检测的作用。</p>

<ul>
<li>driver<br/>
driver 和 replica set之间的通信是通过 <code>db.isMaster()</code> . 以此确定向哪一个db写数据。根据调试经验，向secondary写的数据实际都不会入库。</li>
</ul>


<p><code>ruby
   Mongo::RepelSetConnection.new(['localhost', 40000], ['localhost', 40001], :read =&gt; :secondary)
   # **secondary** 实现了对读的scale
</code></p>

<h3>Sharding</h3>

<ul>
<li><p>几时shard</p>

<ul>
<li>disk activity</li>
<li>system load</li>
<li>ratio of working set size to available RAM.</li>
</ul>
</li>
<li><p>shards<br/>
一个shard其实就是一个replica set，或者mongod</p></li>
<li><p>mongos router<br/>
 访问整个cluster的入口<br/>
将整个系统粘连在一起 <br/>
轻量级的，驻留在内存中的，不写硬盘的</p></li>
<li><p>config server<br/>
处理存储的可靠性，实现两阶段提交<br/>
mongos 的 config 数据是问 config server 获取<br/>
需要3个独立的机器运行才能保证冗余性</p></li>
</ul>


<p>要 shard 则要去确定 <strong>shard key</strong>, key可以是组合的fields。 随后确定key上的值，值之间的区域构成了 <strong>chunk</strong> .</p>

<h3>配置shard的步骤</h3>

<p>启动shard
```bash</p>

<h1>起shard</h1>

<p>mkdir -p /data/rs-a-1
mkdir -p /data/rs-a-2
mkdir -p /data/rs-a-3</p>

<p>mkdir -p /data/rs-b-1
mkdir -p /data/rs-b-2
mkdir -p /data/rs-b-3</p>

<p>numactl --interleave=all mongod --shardsvr  --dbpath /data/rs-a-1 --port 30000 --logpath /data/rs-a-1.log --fork --nojournal</p>

<h1>numactl --interleave=all mongod --shardsvr --replSet shard-a --dbpath /data/rs-a-2 --port 30001 --logpath /data/rs-a-2.log --fork --nojournal</h1>

<h1>numactl --interleave=all mongod --shardsvr --replSet shard-a --dbpath /data/rs-a-3 --port 30002 --logpath /data/rs-a-3.log --fork --nojournal</h1>

<p>numactl --interleave=all mongod --shardsvr  --dbpath /data/rs-b-1 --port 30100 --logpath /data/rs-b-1.log --fork --nojournal</p>

<h1>numactl --interleave=all mongod --shardsvr --replSet shard-b --dbpath /data/rs-b-2 --port 30101 --logpath /data/rs-b-2.log --fork --nojournal</h1>

<h1>numactl --interleave=all mongod --shardsvr --replSet shard-b --dbpath /data/rs-b-3 --port 30102 --logpath /data/rs-b-3.log --fork --nojournal</h1>

<h1>起config server，至少3个，可以和数据库节点的mongod运行在一台服务器上</h1>

<p>mkdir -p /data/config-1
mkdir -p /data/config-2
mkdir -p /data/config-3
numactl --interleave=all mongod --configsvr --dbpath /data/config-1 --port 27019 --logpath /data/config-1.log --fork --nojournal
numactl --interleave=all mongod --configsvr --dbpath /data/config-2 --port 27020 --logpath /data/config-2.log --fork --nojournal
numactl --interleave=all mongod --configsvr --dbpath /data/config-3 --port 27021 --logpath /data/config-3.log --fork --nojournal</p>

<p>sleep 2</p>

<h1>起mongos，os可以在应用服务器上一台装一个</h1>

<p>mongos --configdb localhost:27019,localhost:27020,localhost:27021 --logpath /data/mongos.log --fork --port 40000</p>

<p>```</p>

<p>使用js设置shard
```js
mongo localhost:40000
mongos> sh.addShard("localhost:30000")
mongos> sh.addShard("localhost:30100")
mongos> db.getSiblingDB("config").shards.find()
{ "<em>id" : "shard0000", "host" : "localhost:30000" }
{ "</em>id" : "shard0001", "host" : "localhost:30100" }</p>

<p>mongos> sh.enableSharding("myapp")
mongos> sh.status()</p>

<h1>选择好的sharding key，是设计时的重要考虑对象</h1>

<p>mongos> sh.shardCollection("myapp.pcmds", {c4:1, _id: 1})
mongos> db.getSiblingDB("config").collections.find()
```</p>

<p>观察状态
```js
use config
mongos> db.chunks.count()
1
mongos> db.chunks.find()
{ "<em>id" : "myapp.pcmds-c4_MinKey_id_MinKey", "lastmod" : { "t" : 1000, "i" : 0 }, "ns" : "myapp.pcmds", "min" : { "c4" : { $minKey : 1 }, "</em>id" : { $minKey : 1 } }, "max" : { "c4" : { $maxKey : 1 }, "_id" : { $maxKey : 1 } }, "shard" : "shard0000" }</p>

<p>mongos> db.chunks.count({"shard": "shard0001"})
```</p>

<p>实际<strong>写</strong>操作时分为 <strong>routed</strong> 或 <strong>scattered</strong> 两种模式。<br/>
<strong>读</strong>操作则有 routed / scattered gather / distibuted merge sort.</p>

<p>尽管里面的mongodb进程很多，但是真正数据量大的还是节点本身。config server占用的资源并不多，可以和数据节点共用一台服务器。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Learning Mongodb (1)]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2012/03/29/learning-mongodb-1/"/>
    <updated>2012-03-29T15:59:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2012/03/29/learning-mongodb-1</id>
    <content type="html"><![CDATA[<p>一个不错的mongodb<a href="http://tutorial.mongly.com/tutorial/index">入门游戏</a>. 一共二十课，介绍了基本的内容。通常20-60分钟可以通关。<br/>
下面是同一个作者给mongodb写的<a href="https://github.com/karlseguin/the-little-mongodb-book">小书</a>.<br/>
需要执行的脚本可以这样输入 <code>./mongo server:27017/dbname --quiet my_commands.js</code></p>

<p>table - collection<br/>
row - document<br/>
column - field<br/>
最核心最本质的突破点在于，将定义字段 从table级别  <strong>下放</strong> 到了document的级别，因而起了不同的名字以示区别。</p>

<h3>selector</h3>

<p><code>js
{filed, value}
$lt, $gt, $lte, $gte, $ne
$or
  db.unicorns.find({gender: 'f', $or: [{loves: 'apple'}, {weight: {$lt: 500}}]})
</code></p>

<h3>replace 和$set及modifier</h3>

<p>mongodb的update至少跟两个参数：</p>

<ul>
<li>selector</li>
<li><p><strong>replace</strong>的值</p>

<p>这点区别于sql，他会对document的值做全部更新</p></li>
</ul>


<p>如果只是部分更新，请用<code>$set</code>, <code>db.unicorns.update({name: "ooo"}, {$set: {name: 'dddd'}})</code></p>

<p><code>$inc</code>: <code>db.unicorns.update({name: 'dddd'}, {$inc: {vampires: -2}})</code><br/>
<code>$push</code>: <code>db.unicorns.update({name: 'ddd'}, {$push: {loves: 'sugar'}})</code><br/>
$inc主要用于整型数据，push主要用于数组等类型。</p>

<ul>
<li>upserts<br/>
第3个参数决定是否upserts，即不存在的时候就插入一笔新的数据, 默认为false不开启。</li>
<li>multiple update
第4个参数决定是否批量更新。 默认情况下，只会更新一笔数据，而不是所有满足条件的数据。</li>
</ul>


<p>upserts搭配$inc的威力还是巨大的。</p>

<h3>find进一步</h3>

<p><code>find()</code>返回的仅仅是<code>cursors</code>，以此实现链式反应。</p>

<p><code>find()</code>还有一个参数，就是指定列名。 <code>db.unicorns.find(null, {name:1, _id: 0})</code>.<br/>
  0表示隐藏不要，只有_id能和显示或隐藏混合使用，否则只能使用0或者1</p>

<p><code>sort</code>等于sql里面的order，<code>db.unicorns.find().sort({weight: -1})</code>，1表示升序，-1表示降序。<br/>
对于没有索引的排序，mongodb有最大尺寸的限制，这可以认为是一个瑕疵，但对避免很次的数据库设计有好处。</p>

<p><strong>paging</strong>通过<code>skip()</code>和<code>limit()</code>来配合实现。limit表示返回几条，skip表示略过前面多少个。</p>

<p><code>count()</code>其实也是cursor方法，只不过shell实现了简写。下面两个是等效的：<br/>
<code>db.unicorns.find({vampires: {$gt: 50}}).count</code> &lt;=> <code>db.unicorns.count({vampires: {$gt: 500}})</code></p>

<h3>Data模型设计区别</h3>

<ul>
<li><p>No Joins, 替代品是在client多做一次查询，该查询通常是针对已加索引的字段。</p></li>
<li><p>Arrays 和 Embedded Documents</p>

<ul>
<li>arrays适合处理用于many-to-many的关联，在mongodb中，array的等级很高，使用很多</li>
<li>embedded的doc是的可以进一步反范式化，查询可以用面向对象的方式<code>.</code>进行链式表达.<code>db.employees.find({'family.mother': 'Chani'})</code></li>
<li>上面两个均可选择，当前的限制是一条doc最大的尺寸是4mB</li>
</ul>
</li>
<li><p>DBRef<br/>
等同于外键，是在服务器端支持，但还需客户端支持。</p></li>
<li><p>collection数量考虑<br/>
以schemaless的角度考虑，一个collection实际上可以支持全部的应用。但是，实际上使用时还是会按照sql数据库表格的设计方式进行区分。
当然，many_to_many的表格基本是不存在了。而设计时，可以多考虑使用<strong>embedded</strong>的方式。</p></li>
</ul>


<h3>何时使用MongoDB</h3>

<ul>
<li><p>大部分数据其实也是结构化的，所以schemaless实际上并未受到重视。但是在开发流程方面则发生了很大的变化，而这一点对于从java .net过来的程序员是震撼性的。</p></li>
<li><p>Writes，也就是logging</p>

<ul>
<li>是因为mongodb的写性能很高。因为它允许写命令即可返回而不用等待真实的写。类似non-block。</li>
<li>1.8以后也增加了safe的模式 ，确保写是成功的； 对于违反约束的写，比如唯一索引，不safe的写也不会返回错误</li>
<li><code>capped collection</code>使得文件尺寸可以设置最大值，或条数设置最大值，有利于反复循环利用现有的空间。 <code>db.createCollection('logs', {capped: true, size: 1048576, max: 10000})</code></li>
<li>schemaless对日志来说，也可以较易地做修改</li>
<li><code>journal=true</code>位于<code>mongodb.config</code>中有利于提高durability（耐久性）</li>
</ul>
</li>
<li><p>事务性 <br/>
这并非mongodb的强项。经过有atomic的操作和两段提交的例子。前者用的还是比较多的，如<code>$inc</code>, $<code>set</code></p></li>
<li><p>数据处理</p>

<ul>
<li>内建的js的MapReduce，但是js是单线程的</li>
<li>可以使用Hadoop，也存在MongoDB adapter for Hadoop</li>
</ul>
</li>
<li><p>地图应用<br/>
内建有$near $within 的操作符</p></li>
</ul>


<h3>MapReduce</h3>

<p>尽管 Mongodb 内建 MapReduce 计算模型，自己只要写 hook，然后使用 <code>db.hits.mapReduce(map, reduce, {out: {inline: 1}})</code> 来调用。但是，本质上这其实一种存储过程的写法，用来弥补group计算
功能的不够强大。毕竟js是 <strong>单线程</strong>，所以这里的MapReduce有点鸡肋，聊胜于无。如果真的大数据量处理，还是用Hadoop等吧。<br/>
毕竟MapReduce是一种模式，而各语言实际上有各自的实现。</p>

<h3>性能及其他</h3>

<ul>
<li><p>索引</p>

<ul>
<li>普通索引 <code>db.unicorns.ensureIndex({name: 1})</code>, 1表示升序</li>
<li>unique <code>db.unicorns.ensureIndex({name: 1}, {unique: true})</code></li>
<li>复合 <code>db.unicorns.ensureIndex({name:1, vampires: -1})</code>, -1表示降序，在复合索引中，有意义</li>
<li>explain: 可以查看查询使用的索引状况; <code>db.unicorns.find().explain()</code>, 例如<code>BasiceCursor</code>, <code>BtreeCursor</code></li>
</ul>
</li>
<li><p>Sharding<br/>
MongoDB支持自动partition<br/>
Sharding可以和<strong>Replication</strong>合在一起做，比如每个shard由主从两个mongo组成，中间其实还有一个arbiter</p></li>
<li><p>性能信息</p>

<ul>
<li><code>stats()</code>可以看状态，主要是size方面的信息</li>
<li>web的接口除了浏览器终端，还有restful的接口</li>
<li><code>db.system.profile.find()</code> 可以分析性能</li>
</ul>
</li>
<li><p>备份及恢复
<code>mongodump --db tutorial --out backup</code> 可把一个schema导入到backup这个目录下，里面的内容是相应的 <strong>bson</strong> 文件<br/>
<code>mongorestore --collection pcmds backup/tutorial/pcmds.bson</code> 可以把数据复原<br/>
<code>mongoexport</code> 和 <code>mongoimport</code> 可以导入导出json和csv文件</p></li>
</ul>


<h3>下一步计划</h3>

<ol>
<li><a href="/blog/2012/03/30/learning-mongodb-2/">尝试分布式及复制  （sharding and replication)</a></li>
<li><a href="/blog/2012/04/17/deployment-and-administration-for-mongod/">部署和管理</a></li>
<li>GridFS</li>
<li>Hadoop 的 MapReduce 学习及引用</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[阶级分层]]></title>
    <link href="http://octopresszhangyu.herokuapp.com/blog/2012/03/13/jiejifenchen/"/>
    <updated>2012-03-13T22:05:00+08:00</updated>
    <id>http://octopresszhangyu.herokuapp.com/blog/2012/03/13/jiejifenchen</id>
    <content type="html"><![CDATA[<p>以中国人实际的概念而言，所谓封建社会 就是  <strong>地富</strong> 社会。<br/>
知识分子也开始装老粗，因为农民思维农民习气受到推崇。因为彼时是战斗阶段，农民是地位最高的一个阶层。<br/>
阶级出身论是中共革命最重要的组织原则，也是思想原则。在革命年代有过重要贡献。这和员工治理有很多相通的地方。但是公司怎么折腾明显是私人的事情。一个国家都这样，就必然要出现问题。</p>

<p>苏共也始终对‘<strong>农民化</strong>’的中共党员问题‘忧虑’。于是此后工人阶级的地位迅速上升。因为他们代表着先进。<br/>
工人阶级之后是农民阶级。对农民来说，实行城乡二元结构，制止“自发资本主义倾向”，使农民走集体化道路。政治地位高，经济地位低，因为农民的生产工具不先进，所以不是先进性代表。<br/>
以上二者中的一部分又构成“工农子弟兵”，即军人。<br/>
商、学是暧昧阶层。</p>

<p>谁说毛主席不懂经济，他用金钱来剥夺城市资本家和知识分子的政治地位，对底层不纯分子则辅之以经济收入限制。不提高农民生活水平也有可以继续全盘控制的意思在里面。毕竟经济地位一旦提高，就不再听党的话了。<br/>
“过高的生活水平已使一些工人孵化起来，大隆机器厂工人不愿听共产主义的道理。”使人民保持贫穷，靠命令才更能运行社会。</p>

<p>敌情过分估计导致的纯化观。<br/>
57年后，家庭出生和思想纯正开始两者结合，不单看背景还要看当前的‘思想状态’。<br/>
但这也使得区分很容易出现乾坤大挪移的特点，比如老干部也会突然被划进来。这样就变成所有的阶层都没有安全感。也就是到了这个时候以后，这套东西意味着步入晚期。</p>

<p>地富反坏右五类分子，一开始只有地富反坏四类，右是思想不存在等于血统不纯正。
与此相应，好的分子也有几类。军人 干部 工人 农民是革命四类，因为军人的代表是林彪，干部的代表是刘少奇。所以军人吃香了。工人又靠后排。</p>

<p>以阶级斗争来运行国家，需要反面人物也世袭罔替。所以随着时间的推移和又一代人的发展，阶级出身论终于演化成了阶级血统论。<br/>
但是，这套东西的破产也是在所难免。因为黑色阶级总有被消灭或者失去活力失去新鲜感的一天，即无法满足以阶级斗争为冈来建设国家的要求。这样就又需要不停地将红色分子分化出来重新定义成黑类。那么自然整个的基石就开始动摇，毕竟中国人都不傻。</p>

<p>几个对毛主席的评价:</p>

<p>王明是言必称希腊；毛主席是言必称秦始皇<br/>
拒谏爱谄，多疑善变，言而无信，绵里藏针<br/>
毛主席熟读的不是马列著作，而是二十四史。线装书看得太多。</p>
]]></content>
  </entry>
  
</feed>
